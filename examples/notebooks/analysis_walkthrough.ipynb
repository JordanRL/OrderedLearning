{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# OrderedLearning Analysis Walkthrough\n\nThis notebook demonstrates how to use the OrderedLearning analysis tools to explore experiment results.\nIt uses **synthetic data** so you can run it without completing an actual experiment first.\n\n## What you'll learn\n\n1. How experiment metric data is structured\n2. Loading experiment data with `analysis_tools.data_loader`\n3. Plotting metrics with `analysis_tools.visualize`\n4. Switching between dark and paper (publication-ready) styles\n5. Computing convergence statistics\n6. Building summary tables for comparison\n7. Using the CLI analysis tools (`analyze_experiment.py`)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Experiment Data\n",
    "\n",
    "We'll create fake metric logs that mimic what a real `mod_arithmetic` experiment produces.\n",
    "Two strategies (\"stride\" and \"random\") with loss and accuracy curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Create a temporary output directory\n",
    "output_dir = tempfile.mkdtemp(prefix=\"ol_demo_\")\n",
    "experiment = \"demo_experiment\"\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "for strategy in [\"stride\", \"random\"]:\n",
    "    strat_dir = os.path.join(output_dir, experiment, strategy)\n",
    "    os.makedirs(strat_dir, exist_ok=True)\n",
    "\n",
    "    # Generate synthetic JSONL metrics\n",
    "    with open(os.path.join(strat_dir, f\"{strategy}.jsonl\"), \"w\") as f:\n",
    "        for step in range(0, 5001, 10):\n",
    "            progress = step / 5000\n",
    "            # Stride strategy converges faster\n",
    "            rate = 4.0 if strategy == \"stride\" else 2.5\n",
    "            base_loss = 4.5 * math.exp(-rate * progress) + 0.05\n",
    "            val_acc = min(100.0, 100.0 * (1 - math.exp(-(rate + 0.5) * progress)))\n",
    "            train_acc = min(100.0, 100.0 * (1 - math.exp(-(rate + 1.0) * progress)))\n",
    "\n",
    "            record = {\n",
    "                \"step\": step,\n",
    "                \"hook_point\": \"SNAPSHOT\",\n",
    "                \"training_metrics/loss\": round(base_loss + np.random.normal(0, 0.03), 6),\n",
    "                \"training_metrics/val_acc\": round(val_acc + np.random.normal(0, 0.8), 4),\n",
    "                \"training_metrics/train_acc\": round(train_acc + np.random.normal(0, 0.4), 4),\n",
    "            }\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    # Write minimal experiment_config.json\n",
    "    config = {\n",
    "        \"experiment_name\": experiment,\n",
    "        \"strategy\": strategy,\n",
    "        \"epochs\": 5000,\n",
    "        \"seed\": 42,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_size\": 256,\n",
    "    }\n",
    "    with open(os.path.join(strat_dir, \"experiment_config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Synthetic data created in: {output_dir}\")\n",
    "print(f\"Strategies: stride, random\")\n",
    "print(f\"Steps: 0 to 5000 (every 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Experiment Data\n",
    "\n",
    "The `load_experiment_data()` function discovers strategy directories, loads JSONL (preferred) or CSV files,\n",
    "normalizes columns, and returns a tidy pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_tools.data_loader import load_experiment_data\n",
    "\n",
    "df = load_experiment_data(experiment, output_dir=output_dir)\n",
    "\n",
    "print(f\"Loaded {len(df)} rows across {df['strategy'].nunique()} strategies\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame has one row per (strategy, step) pair. Columns include:\n",
    "- `step` — training step number\n",
    "- `strategy` — which strategy produced this data point\n",
    "- `training_metrics/loss`, `training_metrics/val_acc`, etc. — metric values from hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot Metrics\n",
    "\n",
    "The `analysis_tools.visualize` module provides `OLFigure` for subplot management\n",
    "and `plot_time_series()` for line plots with optional EMA smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from analysis_tools.visualize import ema_smooth\n",
    "from analysis_tools.style import apply_style, STRATEGY_PALETTE\n",
    "\n",
    "# Apply the dark theme (matches the console theme)\n",
    "apply_style(\"dark\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "metrics = [\"training_metrics/loss\", \"training_metrics/val_acc\"]\n",
    "labels = [\"Loss\", \"Validation Accuracy (%)\"]\n",
    "\n",
    "for i, (metric, label) in enumerate(zip(metrics, labels)):\n",
    "    ax = axes[i]\n",
    "    for j, strategy in enumerate([\"stride\", \"random\"]):\n",
    "        sdf = df[df[\"strategy\"] == strategy]\n",
    "        raw = sdf[metric].values\n",
    "        smoothed = ema_smooth(raw, weight=0.9)\n",
    "        # Raw data as faint background\n",
    "        ax.plot(sdf[\"step\"], raw, alpha=0.15, color=STRATEGY_PALETTE[j])\n",
    "        # Smoothed as solid line\n",
    "        ax.plot(sdf[\"step\"], smoothed, label=strategy, color=STRATEGY_PALETTE[j])\n",
    "    ax.set_xlabel(\"Step\")\n",
    "    ax.set_ylabel(label)\n",
    "    ax.legend()\n",
    "    ax.set_title(label)\n",
    "\n",
    "fig.suptitle(\"Demo Experiment: Strategy Comparison\", fontsize=14)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Paper-Ready Style\n",
    "\n",
    "Switch to the `paper` style for publication-quality figures.\n",
    "This uses the Wong colorblind-friendly palette (Nature Methods, 2011)\n",
    "with a white background and clean typography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to paper style\n",
    "apply_style(\"paper\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "for j, strategy in enumerate([\"stride\", \"random\"]):\n",
    "    sdf = df[df[\"strategy\"] == strategy]\n",
    "    smoothed = ema_smooth(sdf[\"training_metrics/val_acc\"].values, weight=0.95)\n",
    "    ax.plot(sdf[\"step\"], smoothed, label=strategy, color=STRATEGY_PALETTE[j], linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel(\"Training Step\")\n",
    "ax.set_ylabel(\"Validation Accuracy (%)\")\n",
    "ax.legend(frameon=True)\n",
    "ax.set_title(\"Convergence Comparison\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Switch back to dark for remaining cells\n",
    "apply_style(\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convergence Analysis\n",
    "\n",
    "Find the training step at which each strategy first reaches a target accuracy threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 95.0\n",
    "\n",
    "print(f\"Convergence analysis: val_acc >= {threshold}%\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for strategy in [\"stride\", \"random\"]:\n",
    "    sdf = df[df[\"strategy\"] == strategy]\n",
    "    above = sdf[sdf[\"training_metrics/val_acc\"] >= threshold]\n",
    "    if len(above) > 0:\n",
    "        first_step = int(above.iloc[0][\"step\"])\n",
    "        print(f\"  {strategy:15s}  reached {threshold}% at step {first_step}\")\n",
    "    else:\n",
    "        print(f\"  {strategy:15s}  did not reach {threshold}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Table\n",
    "\n",
    "Aggregate final metrics across strategies for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final values for each strategy\n",
    "summary_rows = []\n",
    "for strategy in [\"stride\", \"random\"]:\n",
    "    sdf = df[df[\"strategy\"] == strategy]\n",
    "    final = sdf.iloc[-1]\n",
    "    summary_rows.append({\n",
    "        \"Strategy\": strategy,\n",
    "        \"Final Loss\": f\"{final['training_metrics/loss']:.4f}\",\n",
    "        \"Final Val Acc\": f\"{final['training_metrics/val_acc']:.2f}%\",\n",
    "        \"Min Loss\": f\"{sdf['training_metrics/loss'].min():.4f}\",\n",
    "        \"Max Val Acc\": f\"{sdf['training_metrics/val_acc'].max():.2f}%\",\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "summary_df = pd.DataFrame(summary_rows).set_index(\"Strategy\")\n",
    "print(\"Strategy Comparison\")\n",
    "print(\"=\" * 60)\n",
    "print(summary_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7. CLI Analysis Tools\n\nThe `analyze_experiment.py` script provides the same analysis capabilities from the command line.\nThis is useful for batch processing, scripting, or when you prefer not to use a notebook.\n\nAvailable tools: `metric_plot`, `convergence`, `compare`, `correlation`, `layer_dynamics`, `weight_compare`, `export_table`.\n\nAll tools accept `--output-dir` to point at your experiment data. Plots are saved to\n`{output_dir}/{experiment}/analysis/{tool_name}/`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import subprocess\n\n# The project root is two levels up from this notebook\nproject_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n\ndef run_cli(args):\n    \"\"\"Run an analyze_experiment.py command and print the output.\"\"\"\n    cmd = [\"python\", \"analyze_experiment.py\"] + args\n    print(f\"$ {' '.join(cmd)}\\n\")\n    result = subprocess.run(cmd, capture_output=True, text=True, cwd=project_root)\n    if result.stdout:\n        print(result.stdout)\n    if result.returncode != 0 and result.stderr:\n        print(result.stderr)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### metric_plot\n\nPlot loss and accuracy curves with EMA smoothing. This produces the same kind of\noverlay plot we made manually in section 3.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Plot loss and val_acc with smoothing\nrun_cli([\n    \"demo_experiment\", \"metric_plot\",\n    \"--metrics\", \"training_metrics/loss\", \"training_metrics/val_acc\",\n    \"--output-dir\", output_dir,\n    \"--smooth\", \"0.9\",\n])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### convergence\n\nFind when each strategy reaches a target accuracy, and plot a bar chart comparing them.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Time-to-threshold: when does val_acc first reach 95%?\nrun_cli([\n    \"demo_experiment\", \"convergence\",\n    \"--metrics\", \"training_metrics/val_acc\",\n    \"--threshold\", \"95.0\",\n    \"--output-dir\", output_dir,\n    \"--smooth\", \"0.9\",\n])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### export_table\n\nGenerate a Markdown or LaTeX table of final metrics, ready to paste into a paper.\nUse `--bold-best` to highlight the best strategy per metric.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Export a Markdown comparison table with best values bolded\nrun_cli([\n    \"demo_experiment\", \"export_table\",\n    \"--metrics\", \"training_metrics/loss\", \"training_metrics/val_acc\",\n    \"--output-dir\", output_dir,\n    \"--table-format\", \"markdown\",\n    \"--bold-best\",\n])",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Cleanup\n\nRemove the temporary synthetic data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(output_dir)\n",
    "print(f\"Cleaned up: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Run a real experiment:** `python run_experiment.py mod_arithmetic --strategy stride --with-hooks full --hook-jsonl`\n",
    "- **Use the CLI analysis tools:** `python analyze_experiment.py mod_arithmetic metric_plot --metrics training_metrics/loss`\n",
    "- **Read the getting-started guide:** [docs/getting-started.md](../../docs/getting-started.md)\n",
    "- **Explore the hook reference:** [docs/instrumentation-hooks.md](../../docs/instrumentation-hooks.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}